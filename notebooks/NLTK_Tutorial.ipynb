{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK for sentiment analysis\n",
    "Following:  https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords, state_union, gutenberg, wordnet, movie_reviews\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import pprint\n",
    "import random\n",
    "import pickle\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bayes’ theorem was named after Thomas Bayes (1701–1761), who studied how to compute a distribution for the probability parameter of a binomial distribution (in modern terminology).', 'Bayes’ unpublished manuscript was significantly edited by Richard Price before it was posthumously read at the Royal Society.', 'Price edited[3] Bayes’ major work “An Essay towards solving a Problem in the Doctrine of Chances” (1763), which appeared in “Philosophical Transactions,”[4] and contains Bayes’ Theorem.', 'Price wrote an introduction to the paper which provides some of the philosophical basis of Bayesian statistics.', 'In 1765 he was elected a Fellow of the Royal Society in recognition of his work on the legacy of Bayes.']\n",
      "['Bayes', '’', 'theorem', 'was', 'named', 'after', 'Thomas', 'Bayes', '(', '1701–1761', ')', ',', 'who', 'studied', 'how', 'to', 'compute', 'a', 'distribution', 'for', 'the', 'probability', 'parameter', 'of', 'a', 'binomial', 'distribution', '(', 'in', 'modern', 'terminology', ')', '.', 'Bayes', '’', 'unpublished', 'manuscript', 'was', 'significantly', 'edited', 'by', 'Richard', 'Price', 'before', 'it', 'was', 'posthumously', 'read', 'at', 'the', 'Royal', 'Society', '.', 'Price', 'edited', '[', '3', ']', 'Bayes', '’', 'major', 'work', '“', 'An', 'Essay', 'towards', 'solving', 'a', 'Problem', 'in', 'the', 'Doctrine', 'of', 'Chances', '”', '(', '1763', ')', ',', 'which', 'appeared', 'in', '“', 'Philosophical', 'Transactions', ',', '”', '[', '4', ']', 'and', 'contains', 'Bayes', '’', 'Theorem', '.', 'Price', 'wrote', 'an', 'introduction', 'to', 'the', 'paper', 'which', 'provides', 'some', 'of', 'the', 'philosophical', 'basis', 'of', 'Bayesian', 'statistics', '.', 'In', '1765', 'he', 'was', 'elected', 'a', 'Fellow', 'of', 'the', 'Royal', 'Society', 'in', 'recognition', 'of', 'his', 'work', 'on', 'the', 'legacy', 'of', 'Bayes', '.']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Bayes’ theorem was named after Thomas Bayes (1701–1761), who studied how to compute a distribution for the probability parameter of a binomial distribution (in modern terminology). Bayes’ unpublished manuscript was significantly edited by Richard Price before it was posthumously read at the Royal Society. Price edited[3] Bayes’ major work “An Essay towards solving a Problem in the Doctrine of Chances” (1763), which appeared in “Philosophical Transactions,”[4] and contains Bayes’ Theorem. Price wrote an introduction to the paper which provides some of the philosophical basis of Bayesian statistics. In 1765 he was elected a Fellow of the Royal Society in recognition of his work on the legacy of Bayes.\"\n",
    "print(sent_tokenize(example_text))\n",
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "words = word_tokenize(example_text)\n",
    "filtered = [w for w in words if w not in stops]\n",
    "print('ALL WORDS: ', words)\n",
    "print('FILTERED WORDS: ', filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "example_words = ['rider', 'ride', 'riding', 'ridded', 'ridely']\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_sentence = 'I was riding in the ridely vehicle and enjoying the ride from which I had ridded before'\n",
    "for w in word_tokenize(example_sentence):\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of the acronyms for POS tagging:\n",
    "\n",
    "```text\n",
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent's\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PunktSentenceTokenizer tokenizer is capable of unsupervised machine learning, so you can actually train it on any body of text that you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "test_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(test_text)\n",
    "print(tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_sentences(sents):\n",
    "    for sent in sents:\n",
    "        # Tokenize by word and get POS tags for each\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        # Pretty print\n",
    "        pp.pprint(tagged)\n",
    "\n",
    "mysents = tokenized[:2]\n",
    "process_sentences(mysents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_gram = r'''Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}'''\n",
    "def process_sentences_regex(sents, chunk_gram):\n",
    "    for sent in sents:\n",
    "        # Tokenize by word and get POS tags for each\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "        chunked = chunk_parser.parse(tagged)\n",
    "        # Interactive drawing - uncomment if care to see the tree\n",
    "#         chunked.draw()\n",
    "        \n",
    "        \n",
    "mysents = tokenized[:2]\n",
    "process_sentences_regex(mysents, chunk_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This line, broken down:\n",
    "\n",
    "```text\n",
    "<RB.?>* = \"0 or more of any tense of adverb,\" followed by:\n",
    "\n",
    "<VB.?>* = \"0 or more of any tense of verb,\" followed by:\n",
    "\n",
    "<NNP>+ = \"One or more proper nouns,\" followed by\n",
    "\n",
    "<NN>? = \"zero or one singular noun.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_gram = r'''Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}'''\n",
    "chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "def process_sentences_regex(sents, chunk_gram):\n",
    "    for sent in sents:\n",
    "        # Tokenize by word and get POS tags for each\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        chunked = chunk_parser.parse(tagged)\n",
    "        subtreed = chunked.subtrees(filter=lambda t: t.label() == 'Chunk')\n",
    "        for subtree in subtreed:\n",
    "            print(subtree)\n",
    "        \n",
    "        \n",
    "mysents = tokenized[:2]\n",
    "process_sentences_regex(mysents, chunk_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinking with NLTK\n",
    "\n",
    "Chinking is a lot like chunking, it is basically a way for you to remove a chunk from a chunk. The chunk that you remove from your chunk is your chink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_gram = r'''Chunk: {<.*>+}\n",
    "                        }<VB.?|IN|DT|TO>+{'''\n",
    "chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "def process_sentences_regex(sents, chunk_gram):\n",
    "    for sent in sents:\n",
    "        # Tokenize by word and get POS tags for each\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        chunked = chunk_parser.parse(tagged)\n",
    "        subtreed = chunked.subtrees(filter=lambda t: t.label() == 'Chunk')\n",
    "        for subtree in subtreed:\n",
    "            print(subtree)\n",
    "        \n",
    "        \n",
    "mysents = tokenized[:2]\n",
    "process_sentences_regex(mysents, chunk_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we're removing one or more verbs, prepositions, determiners, or the word 'to'.\n",
    "\n",
    "    }<VB.?|IN|DT|TO>+{"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "test_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(test_text)\n",
    "\n",
    "def process_sentences_ne(sents, ne_filter='NE', binary=True):\n",
    "    \"\"\"This function finds named entities in sentences by tokenizing\n",
    "       them into words, labeling the parts of speech (POS tags) and\n",
    "       chunking by named entities.\n",
    "       \n",
    "       Parameters\n",
    "       ----------\n",
    "       \n",
    "       sents : list of str\n",
    "           List of sentences pre-tokenized\n",
    "       \n",
    "       ne_filter : str\n",
    "           String to filter out the named entities by their tag which\n",
    "           is either None or 'NE'\n",
    "           \n",
    "       binary : bool\n",
    "          True or False.  Indicates whether to tag by simply 'NE' (True)\n",
    "          or indicate the class of the named entity (False).\n",
    "          \n",
    "       Returns\n",
    "       -------\n",
    "       \n",
    "       No return value.\n",
    "       \n",
    "    \"\"\"\n",
    "    for sent in sents:\n",
    "        # Tokenize by word and get POS tags for each\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        # Extract named entities\n",
    "        named_ent = nltk.ne_chunk(tagged, binary=binary)\n",
    "        if ne_filter is not None:\n",
    "            subtreed = named_ent.subtrees(filter=lambda t: t.label() == ne_filter)\n",
    "        else:\n",
    "            subtreed = named_ent.subtrees()\n",
    "        for subtree in subtreed:\n",
    "            print(subtree)\n",
    "        \n",
    "        \n",
    "mysents = tokenized[5:7]\n",
    "process_sentences_ne(mysents, ne_filter=None, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NE Type and Examples (returned when binary is set to False):**\n",
    "\n",
    "```\n",
    "ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "PERSON - Eddy Bonte, President Obama\n",
    "LOCATION - Murray River, Mount Everest\n",
    "DATE - June, 2008-06-29\n",
    "TIME - two fifty a m, 1:30 p.m.\n",
    "MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "PERCENT - twenty pct, 18.75 %\n",
    "FACILITY - Washington Monument, Stonehenge\n",
    "GPE - South East Asia, Midlothian\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing\n",
    "\n",
    "A very similar operation to stemming is called lemmatizing. The major difference between these is, as you saw earlier, stemming can often create non-existent words, whereas lemmas are actual words.\n",
    "\n",
    "Using from `nltk.stem import WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print('tabbies -> ', lemmatizer.lemmatize('tabbies'))\n",
    "print('cacti -> ', lemmatizer.lemmatize('cacti'))\n",
    "print('better -> ', lemmatizer.lemmatize('better', pos='a'))\n",
    "print('best -> ', lemmatizer.lemmatize('best', pos='a'))\n",
    "print('ran -> ', lemmatizer.lemmatize('ran'))\n",
    "print('ran -> ', lemmatizer.lemmatize('ran', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to the corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_text = gutenberg.raw(\"bible-kjv.txt\")\n",
    "tokenized = sent_tokenize(sample_text)\n",
    "pp.pprint(tokenized[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a list of the corpora see http://www.nltk.org/book/ch02.html#tab-corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet\n",
    "\n",
    "WordNet is a lexical database for the English language, which was created by Princeton, and is part of the NLTK corpus.\n",
    "\n",
    "You can use WordNet alongside the NLTK module to find the meanings of words, synonyms, antonyms, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(\"program\")\n",
    "# Synset\n",
    "synset = syns[0]\n",
    "print(synset.name())\n",
    "# Just the word\n",
    "print(synset.lemmas()[0].name())\n",
    "# Definition\n",
    "print(synset.definition())\n",
    "# Examples\n",
    "print(synset.examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_synsets = wordnet.synsets('good')\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in _synsets:\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        ants = l.antonyms()\n",
    "        if ants:\n",
    "            antonyms.append(ants[0].name())\n",
    "            \n",
    "print('SYNONYMS: ', set(synonyms))\n",
    "print('ANTONYMS: ', set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can also easily use WordNet to compare the similarity of two words and their tenses, by incorporating the Wu and Palmer method for semantic related-ness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('sail.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('truck.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('cat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification and converting words to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In each category (pos/neg) take all file ids (reviews), and\n",
    "# store the tokenized words for that file id along with\n",
    "# pos/neg label:\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the documents for training and testing\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First document in the list\n",
    "print(documents[0])\n",
    "\n",
    "# Make a list of all words, lowercase\n",
    "all_words = [w.lower() for w in movie_reviews.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Frequency distribution\n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top 15 words\n",
    "print(all_words.most_common(15))\n",
    "\n",
    "# Count of a word\n",
    "print(all_words['cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top someodd words\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "# Returns a dict with the word and its presence (bool) in document\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    features = {w: w in words for w in word_features}\n",
    "    return features\n",
    "\n",
    "print(find_features(movie_reviews.words('neg/cv000_29416.txt')))\n",
    "# List of words and their presence in document along with label for that document\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "print(featuresets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and saving naive Bayes classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split into training and test set\n",
    "\n",
    "training_set, test_set = featuresets[:1900], featuresets[1900:]\n",
    "\n",
    "# Train\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# Test\n",
    "print(\"Classifier accuracy  = \", (nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What are the most valuable words when it comes to the neg/pos labels\n",
    "# You'll see the ratio of occurrences neg:pos or pos:neg, depending, to \n",
    "# see how often a word appears in one label as compared to the other.\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('naivebayes.pickle', 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('naivebayes.pickle', 'rb') as f:\n",
    "    classifier = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test on one document (#5) without the label (just the first element of our tuple)\n",
    "classifier.classify(test_set[4][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK and Scikit-Learn\n",
    "There's an API made by the nltk folks for leveraging sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnb_classifer = SklearnClassifier(MultinomialNB())\n",
    "mnb_classifer.train(training_set)\n",
    "print('MultinomialNB accuracy = ',\n",
    "      nltk.classify.accuracy(mnb_classifer, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bnb_classifer = SklearnClassifier(BernoulliNB())\n",
    "bnb_classifer.train(training_set)\n",
    "print('BernoulliNB accuracy = ',\n",
    "      nltk.classify.accuracy(bnb_classifer, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_classifier = SklearnClassifier(LogisticRegression())\n",
    "lr_classifier.train(training_set)\n",
    "print('LogisticRegression accuracy = ',\n",
    "      nltk.classify.accuracy(lr_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_classifier = SklearnClassifier(SGDClassifier())\n",
    "sgd_classifier.train(training_set)\n",
    "print('SGDClassifier accuracy = ',\n",
    "      nltk.classify.accuracy(sgd_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc_classifier = SklearnClassifier(SVC())\n",
    "svc_classifier.train(training_set)\n",
    "print('SVC accuracy = ',\n",
    "      nltk.classify.accuracy(svc_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linearsvc_classifier = SklearnClassifier(LinearSVC())\n",
    "linearsvc_classifier.train(training_set)\n",
    "print('LinearSVC accuracy = ',\n",
    "      nltk.classify.accuracy(linearsvc_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nusvc_classifier = SklearnClassifier(NuSVC())\n",
    "nusvc_classifier.train(training_set)\n",
    "print('NuSVC accuracy = ',\n",
    "      nltk.classify.accuracy(nusvc_classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "        \n",
    "    def classify(self, features):\n",
    "        \"\"\"Iterates through classifiers and classifies\n",
    "           based on features, returning the most popular \"vote\"\n",
    "           or classification.\"\"\"\n",
    "        votes = [c.classify(features) for c in self._classifiers]\n",
    "        try:\n",
    "            m = mode(votes)\n",
    "        except Exception as e:\n",
    "            m = None\n",
    "        return m\n",
    "    \n",
    "    def vote_ratio(self, features):\n",
    "        \"\"\"For or against.\"\"\"\n",
    "        votes = [c.classify(features) for c in self._classifiers]\n",
    "        try:\n",
    "            vote_ratio = votes.count(mode(votes)) / len(votes)\n",
    "        except Exception as e:\n",
    "            vote_ratio = None\n",
    "        return vote_ratio\n",
    "\n",
    "def find_features(document):\n",
    "    \"\"\"Returns a dict with the word and its presence (bool) in document\"\"\"\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    features = {w: w in words for w in word_features}\n",
    "    return features\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy  =  0.64\n",
      "MultinomialNB accuracy =  0.63\n",
      "BernoulliNB accuracy =  0.65\n",
      "LogisticRegression accuracy =  0.67\n",
      "SGDClassifier accuracy =  0.64\n",
      "SVC accuracy =  0.45\n",
      "LinearSVC accuracy =  0.66\n",
      "NuSVC accuracy =  0.63\n",
      "voted_classifier accuracy =  0.63\n",
      "Classification: pos Vote ratio: 1.0\n",
      "Classification: pos Vote ratio: 0.875\n",
      "Classification: pos Vote ratio: 0.875\n",
      "Classification: neg Vote ratio: 0.875\n",
      "Classification: pos Vote ratio: 0.875\n",
      "Classification: neg Vote ratio: 0.875\n"
     ]
    }
   ],
   "source": [
    "     \n",
    "# In each category (pos/neg) take all file ids (reviews), and\n",
    "# store the tokenized words for that file id along with\n",
    "# pos/neg label:\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the documents for training and testing\n",
    "random.shuffle(documents)\n",
    "\n",
    "# List of words in movie reviews\n",
    "all_words = [w.lower() for w in movie_reviews.words()]\n",
    "\n",
    "# Frequency distribution\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "# Top someodd words\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "# List of words and their presence in document along with label for that document\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "# Split into test and training data sets\n",
    "training_set, test_set = featuresets[:1900], featuresets[1900:]\n",
    "\n",
    "# Reload model\n",
    "with open('naivebayes.pickle', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "    \n",
    "\n",
    "# Train and test\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier accuracy  = \", (nltk.classify.accuracy(classifier, test_set)))\n",
    "\n",
    "\n",
    "mnb_classifer = SklearnClassifier(MultinomialNB())\n",
    "mnb_classifer.train(training_set)\n",
    "print('MultinomialNB accuracy = ',\n",
    "      nltk.classify.accuracy(mnb_classifer, test_set))\n",
    "                                  \n",
    "bnb_classifer = SklearnClassifier(BernoulliNB())\n",
    "bnb_classifer.train(training_set)\n",
    "print('BernoulliNB accuracy = ',\n",
    "      nltk.classify.accuracy(bnb_classifer, test_set))\n",
    "\n",
    "lr_classifier = SklearnClassifier(LogisticRegression())\n",
    "lr_classifier.train(training_set)\n",
    "print('LogisticRegression accuracy = ',\n",
    "      nltk.classify.accuracy(lr_classifier, test_set))\n",
    "\n",
    "sgd_classifier = SklearnClassifier(SGDClassifier())\n",
    "sgd_classifier.train(training_set)\n",
    "print('SGDClassifier accuracy = ',\n",
    "      nltk.classify.accuracy(sgd_classifier, test_set))\n",
    "                                  \n",
    "svc_classifier = SklearnClassifier(SVC())\n",
    "svc_classifier.train(training_set)\n",
    "print('SVC accuracy = ',\n",
    "      nltk.classify.accuracy(svc_classifier, test_set))\n",
    "                                  \n",
    "linearsvc_classifier = SklearnClassifier(LinearSVC())\n",
    "linearsvc_classifier.train(training_set)\n",
    "print('LinearSVC accuracy = ',\n",
    "      nltk.classify.accuracy(linearsvc_classifier, test_set))\n",
    "                                  \n",
    "nusvc_classifier = SklearnClassifier(NuSVC())\n",
    "nusvc_classifier.train(training_set)\n",
    "print('NuSVC accuracy = ',\n",
    "      nltk.classify.accuracy(nusvc_classifier, test_set))\n",
    "\n",
    "# Voting\n",
    "\n",
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  mnb_classifer,\n",
    "                                  bnb_classifer,\n",
    "                                  lr_classifier,\n",
    "                                  sgd_classifier,\n",
    "                                  svc_classifier,\n",
    "                                  linearsvc_classifier,\n",
    "                                  nusvc_classifier)\n",
    "\n",
    "print('voted_classifier accuracy = ', (nltk.classify.accuracy(voted_classifier, test_set)))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[0][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[0][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[1][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[1][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[2][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[2][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[3][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[3][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[4][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[4][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[5][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[5][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short reviews - full code sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy  =  0.5078031212484994\n",
      "Most Informative Features\n",
      "                       4 = True              neg : pos    =      2.5 : 1.0\n",
      "                       a = False             neg : pos    =      1.8 : 1.0\n",
      "                       1 = True              neg : pos    =      1.6 : 1.0\n",
      "                       ! = True              neg : pos    =      1.6 : 1.0\n",
      "                       r = False             neg : pos    =      1.5 : 1.0\n",
      "                       9 = True              neg : pos    =      1.4 : 1.0\n",
      "                       6 = True              neg : pos    =      1.3 : 1.0\n",
      "                       [ = True              neg : pos    =      1.2 : 1.0\n",
      "                       u = False             neg : pos    =      1.1 : 1.0\n",
      "                       o = False             pos : neg    =      1.1 : 1.0\n",
      "                       : = True              pos : neg    =      1.1 : 1.0\n",
      "                       u = True              pos : neg    =      1.0 : 1.0\n",
      "                       1 = False             pos : neg    =      1.0 : 1.0\n",
      "                       r = True              pos : neg    =      1.0 : 1.0\n",
      "                       a = True              pos : neg    =      1.0 : 1.0\n",
      "MultinomialNB accuracy =  0.5018007202881153\n",
      "BernoulliNB accuracy =  0.5066026410564226\n",
      "LogisticRegression accuracy =  0.5057022809123649\n",
      "SGDClassifier accuracy =  0.5025510204081632\n",
      "SVC accuracy =  0.4995498199279712\n",
      "LinearSVC accuracy =  0.5055522208883554\n",
      "NuSVC accuracy =  0.49864945978391356\n",
      "voted_classifier accuracy =  0.5007503001200481\n",
      "A few test sample predictions...\n",
      "Classification: neg Vote ratio: 1.0\n",
      "Classification: pos Vote ratio: 0.875\n",
      "Classification: pos Vote ratio: 0.875\n",
      "Classification: pos Vote ratio: 0.875\n",
      "Classification: pos Vote ratio: 0.875\n",
      "Classification: pos Vote ratio: 0.875\n"
     ]
    }
   ],
   "source": [
    "with open('../data/positive.txt', encoding='latin-1', mode='r') as f:\n",
    "    short_pos = f.read()\n",
    "with open('../data/negative.txt', encoding='latin-1', mode='r') as f:\n",
    "    short_neg = f.read()\n",
    "    \n",
    "documents = []\n",
    "\n",
    "for r in short_pos.split('\\n'):\n",
    "    documents.append( (r, 'pos') )\n",
    "for r in short_neg.split('\\n'):\n",
    "    documents.append( (r, 'neg') )\n",
    "\n",
    "all_words = []\n",
    "\n",
    "short_pos_words = word_tokenize(short_pos.replace('\\n', ' '))\n",
    "short_neg_words = word_tokenize(short_neg.replace('\\n', ' '))\n",
    "\n",
    "for w in short_pos_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "for w in short_neg_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "        \n",
    "    def classify(self, features):\n",
    "        \"\"\"Iterates through classifiers and classifies\n",
    "           based on features, returning the most popular \"vote\"\n",
    "           or classification.\"\"\"\n",
    "        votes = [c.classify(features) for c in self._classifiers]\n",
    "        try:\n",
    "            m = mode(votes)\n",
    "        except Exception as e:\n",
    "            m = None\n",
    "        return m\n",
    "    \n",
    "    def vote_ratio(self, features):\n",
    "        \"\"\"For or against.\"\"\"\n",
    "        votes = [c.classify(features) for c in self._classifiers]\n",
    "        try:\n",
    "            vote_ratio = votes.count(mode(votes)) / len(votes)\n",
    "        except Exception as e:\n",
    "            vote_ratio = None\n",
    "        return vote_ratio\n",
    "\n",
    "def find_features(document):\n",
    "    \"\"\"Returns a dict with the word and its presence (bool) in document\"\"\"\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    features = {w: w in words for w in word_features}\n",
    "    return features\n",
    "\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "# Split into test and training data sets\n",
    "training_set, test_set = featuresets[:4000], featuresets[4000:]\n",
    "\n",
    "# Reload model\n",
    "with open('naivebayes.pickle', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "    \n",
    "\n",
    "# Train and test\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier accuracy  = \", (nltk.classify.accuracy(classifier, test_set)))\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "mnb_classifer = SklearnClassifier(MultinomialNB())\n",
    "mnb_classifer.train(training_set)\n",
    "print('MultinomialNB accuracy = ',\n",
    "      nltk.classify.accuracy(mnb_classifer, test_set))\n",
    "                                  \n",
    "bnb_classifer = SklearnClassifier(BernoulliNB())\n",
    "bnb_classifer.train(training_set)\n",
    "print('BernoulliNB accuracy = ',\n",
    "      nltk.classify.accuracy(bnb_classifer, test_set))\n",
    "\n",
    "lr_classifier = SklearnClassifier(LogisticRegression())\n",
    "lr_classifier.train(training_set)\n",
    "print('LogisticRegression accuracy = ',\n",
    "      nltk.classify.accuracy(lr_classifier, test_set))\n",
    "\n",
    "sgd_classifier = SklearnClassifier(SGDClassifier())\n",
    "sgd_classifier.train(training_set)\n",
    "print('SGDClassifier accuracy = ',\n",
    "      nltk.classify.accuracy(sgd_classifier, test_set))\n",
    "                                  \n",
    "svc_classifier = SklearnClassifier(SVC())\n",
    "svc_classifier.train(training_set)\n",
    "print('SVC accuracy = ',\n",
    "      nltk.classify.accuracy(svc_classifier, test_set))\n",
    "                                  \n",
    "linearsvc_classifier = SklearnClassifier(LinearSVC())\n",
    "linearsvc_classifier.train(training_set)\n",
    "print('LinearSVC accuracy = ',\n",
    "      nltk.classify.accuracy(linearsvc_classifier, test_set))\n",
    "                                  \n",
    "nusvc_classifier = SklearnClassifier(NuSVC())\n",
    "nusvc_classifier.train(training_set)\n",
    "print('NuSVC accuracy = ',\n",
    "      nltk.classify.accuracy(nusvc_classifier, test_set))\n",
    "\n",
    "# Voting\n",
    "\n",
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  mnb_classifer,\n",
    "                                  bnb_classifer,\n",
    "                                  lr_classifier,\n",
    "                                  sgd_classifier,\n",
    "                                  svc_classifier,\n",
    "                                  linearsvc_classifier,\n",
    "                                  nusvc_classifier)\n",
    "\n",
    "print('voted_classifier accuracy = ', (nltk.classify.accuracy(voted_classifier, test_set)))\n",
    "\n",
    "print(\"A few test sample predictions...\")\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[0][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[0][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[1][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[1][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[2][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[2][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[3][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[3][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[4][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[4][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[5][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[5][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few test sample predictions...\n",
      "Classification: neg Vote ratio: 0.625\n",
      "neg\n",
      "Classification: neg Vote ratio: 0.625\n",
      "pos\n",
      "Classification: neg Vote ratio: 0.625\n",
      "pos\n",
      "Classification: neg Vote ratio: 0.625\n",
      "pos\n",
      "Classification: neg Vote ratio: 0.625\n",
      "pos\n",
      "Classification: neg Vote ratio: 0.625\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "print(\"A few test sample predictions...\")\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[10][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[0][0]))\n",
    "print(test_set[0][1])\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[11][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[1][0]))\n",
    "print(test_set[1][1])\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[12][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[2][0]))\n",
    "print(test_set[2][1])\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[13][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[3][0]))\n",
    "print(test_set[3][1])\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[14][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[4][0]))\n",
    "print(test_set[4][1])\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[15][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[5][0]))\n",
    "print(test_set[5][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
