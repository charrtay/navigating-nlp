{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK for sentiment analysis\n",
    "Following:  https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords, state_union, gutenberg, wordnet, movie_reviews\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import pprint\n",
    "import random\n",
    "import pickle\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Next week my family and I are going to Yosemite National Park.', 'Ansel Adams took many great pictures of Yosemite.', \"We'll try to climb Half Dome.\", \"It's a rather long hike and we hope that the weather holds.\", 'Mr. S. Harris is my father and Mrs. M. Harris is my mother.']\n",
      "['Next', 'week', 'my', 'family', 'and', 'I', 'are', 'going', 'to', 'Yosemite', 'National', 'Park', '.', 'Ansel', 'Adams', 'took', 'many', 'great', 'pictures', 'of', 'Yosemite', '.', 'We', \"'ll\", 'try', 'to', 'climb', 'Half', 'Dome', '.', 'It', \"'s\", 'a', 'rather', 'long', 'hike', 'and', 'we', 'hope', 'that', 'the', 'weather', 'holds', '.', 'Mr.', 'S.', 'Harris', 'is', 'my', 'father', 'and', 'Mrs.', 'M.', 'Harris', 'is', 'my', 'mother', '.']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Next week my family and I are going to Yosemite National Park.  Ansel Adams took many great pictures of Yosemite.  We'll try to climb Half Dome.  It's a rather long hike and we hope that the weather holds.  Mr. S. Harris is my father and Mrs. M. Harris is my mother.\"\n",
    "print(sent_tokenize(example_text))\n",
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " 'has',\n",
       " 'hasn',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " 'she',\n",
       " 'should',\n",
       " 'shouldn',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " 'wouldn',\n",
       " 'y',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL WORDS:  ['Next', 'week', 'my', 'family', 'and', 'I', 'are', 'going', 'to', 'Yosemite', 'National', 'Park', '.', 'Ansel', 'Adams', 'took', 'many', 'great', 'pictures', 'of', 'Yosemite', '.', 'We', \"'ll\", 'try', 'to', 'climb', 'Half', 'Dome', '.', 'It', \"'s\", 'a', 'rather', 'long', 'hike', 'and', 'we', 'hope', 'that', 'the', 'weather', 'holds', '.', 'Mr.', 'S.', 'Harris', 'is', 'my', 'father', 'and', 'Mrs.', 'M.', 'Harris', 'is', 'my', 'mother', '.']\n",
      "FILTERED WORDS:  ['Next', 'week', 'family', 'I', 'going', 'Yosemite', 'National', 'Park', '.', 'Ansel', 'Adams', 'took', 'many', 'great', 'pictures', 'Yosemite', '.', 'We', \"'ll\", 'try', 'climb', 'Half', 'Dome', '.', 'It', \"'s\", 'rather', 'long', 'hike', 'hope', 'weather', 'holds', '.', 'Mr.', 'S.', 'Harris', 'father', 'Mrs.', 'M.', 'Harris', 'mother', '.']\n"
     ]
    }
   ],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "words = word_tokenize(example_text)\n",
    "filtered = [w for w in words if w not in stops]\n",
    "print('ALL WORDS: ', words)\n",
    "print('FILTERED WORDS: ', filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rider\n",
      "ride\n",
      "ride\n",
      "rid\n",
      "ride\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "example_words = ['rider', 'ride', 'riding', 'ridded', 'ridely']\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "wa\n",
      "ride\n",
      "in\n",
      "the\n",
      "ride\n",
      "vehicl\n",
      "and\n",
      "enjoy\n",
      "the\n",
      "ride\n",
      "from\n",
      "which\n",
      "I\n",
      "had\n",
      "rid\n",
      "befor\n"
     ]
    }
   ],
   "source": [
    "example_sentence = 'I was riding in the ridely vehicle and enjoying the ride from which I had ridded before'\n",
    "for w in word_tokenize(example_sentence):\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of the acronyms for POS tagging:\n",
    "\n",
    "```text\n",
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent's\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PunktSentenceTokenizer tokenizer is capable of unsupervised machine learning, so you can actually train it on any body of text that you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "test_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\\n \\nJanuary 31, 2006\\n\\nTHE PRESIDENT: Thank you all.\", 'Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream.', 'Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King.']\n"
     ]
    }
   ],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(test_text)\n",
    "print(tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('PRESIDENT', 'NNP'),\n",
      "    ('GEORGE', 'NNP'),\n",
      "    ('W.', 'NNP'),\n",
      "    ('BUSH', 'NNP'),\n",
      "    (\"'S\", 'POS'),\n",
      "    ('ADDRESS', 'NNP'),\n",
      "    ('BEFORE', 'IN'),\n",
      "    ('A', 'NNP'),\n",
      "    ('JOINT', 'NNP'),\n",
      "    ('SESSION', 'NNP'),\n",
      "    ('OF', 'IN'),\n",
      "    ('THE', 'NNP'),\n",
      "    ('CONGRESS', 'NNP'),\n",
      "    ('ON', 'NNP'),\n",
      "    ('THE', 'NNP'),\n",
      "    ('STATE', 'NNP'),\n",
      "    ('OF', 'IN'),\n",
      "    ('THE', 'NNP'),\n",
      "    ('UNION', 'NNP'),\n",
      "    ('January', 'NNP'),\n",
      "    ('31', 'CD'),\n",
      "    (',', ','),\n",
      "    ('2006', 'CD'),\n",
      "    ('THE', 'NNP'),\n",
      "    ('PRESIDENT', 'NNP'),\n",
      "    (':', ':'),\n",
      "    ('Thank', 'NNP'),\n",
      "    ('you', 'PRP'),\n",
      "    ('all', 'DT'),\n",
      "    ('.', '.')]\n",
      "[   ('Mr.', 'NNP'),\n",
      "    ('Speaker', 'NNP'),\n",
      "    (',', ','),\n",
      "    ('Vice', 'NNP'),\n",
      "    ('President', 'NNP'),\n",
      "    ('Cheney', 'NNP'),\n",
      "    (',', ','),\n",
      "    ('members', 'NNS'),\n",
      "    ('of', 'IN'),\n",
      "    ('Congress', 'NNP'),\n",
      "    (',', ','),\n",
      "    ('members', 'NNS'),\n",
      "    ('of', 'IN'),\n",
      "    ('the', 'DT'),\n",
      "    ('Supreme', 'NNP'),\n",
      "    ('Court', 'NNP'),\n",
      "    ('and', 'CC'),\n",
      "    ('diplomatic', 'JJ'),\n",
      "    ('corps', 'NN'),\n",
      "    (',', ','),\n",
      "    ('distinguished', 'JJ'),\n",
      "    ('guests', 'NNS'),\n",
      "    (',', ','),\n",
      "    ('and', 'CC'),\n",
      "    ('fellow', 'JJ'),\n",
      "    ('citizens', 'NNS'),\n",
      "    (':', ':'),\n",
      "    ('Today', 'VB'),\n",
      "    ('our', 'PRP$'),\n",
      "    ('nation', 'NN'),\n",
      "    ('lost', 'VBD'),\n",
      "    ('a', 'DT'),\n",
      "    ('beloved', 'VBN'),\n",
      "    (',', ','),\n",
      "    ('graceful', 'JJ'),\n",
      "    (',', ','),\n",
      "    ('courageous', 'JJ'),\n",
      "    ('woman', 'NN'),\n",
      "    ('who', 'WP'),\n",
      "    ('called', 'VBD'),\n",
      "    ('America', 'NNP'),\n",
      "    ('to', 'TO'),\n",
      "    ('its', 'PRP$'),\n",
      "    ('founding', 'NN'),\n",
      "    ('ideals', 'NNS'),\n",
      "    ('and', 'CC'),\n",
      "    ('carried', 'VBD'),\n",
      "    ('on', 'IN'),\n",
      "    ('a', 'DT'),\n",
      "    ('noble', 'JJ'),\n",
      "    ('dream', 'NN'),\n",
      "    ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def process_sentences(sents):\n",
    "    for sent in sents:\n",
    "        # Tokenize by word and get POS tags for each\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        # Pretty print\n",
    "        pp.pprint(tagged)\n",
    "\n",
    "mysents = tokenized[:2]\n",
    "process_sentences(mysents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_gram = r'''Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}'''\n",
    "def process_sentences_regex(sents, chunk_gram):\n",
    "    for sent in sents:\n",
    "        # Tokenize by word and get POS tags for each\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "        chunked = chunk_parser.parse(tagged)\n",
    "        # Interactive drawing - uncomment if care to see the tree\n",
    "#         chunked.draw()\n",
    "        \n",
    "        \n",
    "mysents = tokenized[:2]\n",
    "process_sentences_regex(mysents, chunk_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This line, broken down:\n",
    "\n",
    "```text\n",
    "<RB.?>* = \"0 or more of any tense of adverb,\" followed by:\n",
    "\n",
    "<VB.?>* = \"0 or more of any tense of verb,\" followed by:\n",
    "\n",
    "<NNP>+ = \"One or more proper nouns,\" followed by\n",
    "\n",
    "<NN>? = \"zero or one singular noun.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "(Chunk ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk THE/NNP UNION/NNP January/NNP)\n",
      "(Chunk THE/NNP PRESIDENT/NNP)\n",
      "(Chunk Thank/NNP)\n",
      "(Chunk Mr./NNP Speaker/NNP)\n",
      "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk called/VBD America/NNP)\n"
     ]
    }
   ],
   "source": [
    "chunk_gram = r'''Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}'''\n",
    "chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "def process_sentences_regex(sents, chunk_gram):\n",
    "    for sent in sents:\n",
    "        # Tokenize by word and get POS tags for each\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        chunked = chunk_parser.parse(tagged)\n",
    "        subtreed = chunked.subtrees(filter=lambda t: t.label() == 'Chunk')\n",
    "        for subtree in subtreed:\n",
    "            print(subtree)\n",
    "        \n",
    "        \n",
    "mysents = tokenized[:2]\n",
    "process_sentences_regex(mysents, chunk_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinking with NLTK\n",
    "\n",
    "Chinking is a lot like chunking, it is basically a way for you to remove a chunk from a chunk. The chunk that you remove from your chunk is your chink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP 'S/POS ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk\n",
      "  THE/NNP\n",
      "  UNION/NNP\n",
      "  January/NNP\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  THE/NNP\n",
      "  PRESIDENT/NNP\n",
      "  :/:\n",
      "  Thank/NNP\n",
      "  you/PRP)\n",
      "(Chunk ./.)\n",
      "(Chunk\n",
      "  Mr./NNP\n",
      "  Speaker/NNP\n",
      "  ,/,\n",
      "  Vice/NNP\n",
      "  President/NNP\n",
      "  Cheney/NNP\n",
      "  ,/,\n",
      "  members/NNS)\n",
      "(Chunk Congress/NNP ,/, members/NNS)\n",
      "(Chunk\n",
      "  Supreme/NNP\n",
      "  Court/NNP\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:)\n",
      "(Chunk our/PRP$ nation/NN)\n",
      "(Chunk ,/, graceful/JJ ,/, courageous/JJ woman/NN who/WP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk its/PRP$ founding/NN ideals/NNS and/CC)\n",
      "(Chunk noble/JJ dream/NN ./.)\n"
     ]
    }
   ],
   "source": [
    "chunk_gram = r'''Chunk: {<.*>+}\n",
    "                        }<VB.?|IN|DT|TO>+{'''\n",
    "chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "def process_sentences_regex(sents, chunk_gram):\n",
    "    for sent in sents:\n",
    "        # Tokenize by word and get POS tags for each\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        chunked = chunk_parser.parse(tagged)\n",
    "        subtreed = chunked.subtrees(filter=lambda t: t.label() == 'Chunk')\n",
    "        for subtree in subtreed:\n",
    "            print(subtree)\n",
    "        \n",
    "        \n",
    "mysents = tokenized[:2]\n",
    "process_sentences_regex(mysents, chunk_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we're removing one or more verbs, prepositions, determiners, or the word 'to'.\n",
    "\n",
    "    }<VB.?|IN|DT|TO>+{"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S 31/CD ,/, 2006/CD ./.)\n",
      "(S\n",
      "  (FACILITY White/NNP)\n",
      "  (ORGANIZATION House/NNP)\n",
      "  photo/NN\n",
      "  by/IN\n",
      "  (PERSON Eric/NNP)\n",
      "  DraperEvery/NNP\n",
      "  time/NN\n",
      "  I/PRP\n",
      "  'm/VBP\n",
      "  invited/JJ\n",
      "  to/TO\n",
      "  this/DT\n",
      "  rostrum/NN\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  'm/VBP\n",
      "  humbled/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  privilege/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  mindful/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  history/NN\n",
      "  we/PRP\n",
      "  've/VBP\n",
      "  seen/VBN\n",
      "  together/RB\n",
      "  ./.)\n",
      "(FACILITY White/NNP)\n",
      "(ORGANIZATION House/NNP)\n",
      "(PERSON Eric/NNP)\n"
     ]
    }
   ],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "test_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(test_text)\n",
    "\n",
    "def process_sentences_ne(sents, ne_filter='NE', binary=True):\n",
    "    \"\"\"This function finds named entities in sentences by tokenizing\n",
    "       them into words, labeling the parts of speech (POS tags) and\n",
    "       chunking by named entities.\n",
    "       \n",
    "       Parameters\n",
    "       ----------\n",
    "       \n",
    "       sents : list of str\n",
    "           List of sentences pre-tokenized\n",
    "       \n",
    "       ne_filter : str\n",
    "           String to filter out the named entities by their tag which\n",
    "           is either None or 'NE'\n",
    "           \n",
    "       binary : bool\n",
    "          True or False.  Indicates whether to tag by simply 'NE' (True)\n",
    "          or indicate the class of the named entity (False).\n",
    "          \n",
    "       Returns\n",
    "       -------\n",
    "       \n",
    "       No return value.\n",
    "       \n",
    "    \"\"\"\n",
    "    for sent in sents:\n",
    "        # Tokenize by word and get POS tags for each\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        # Extract named entities\n",
    "        named_ent = nltk.ne_chunk(tagged, binary=binary)\n",
    "        if ne_filter is not None:\n",
    "            subtreed = named_ent.subtrees(filter=lambda t: t.label() == ne_filter)\n",
    "        else:\n",
    "            subtreed = named_ent.subtrees()\n",
    "        for subtree in subtreed:\n",
    "            print(subtree)\n",
    "        \n",
    "        \n",
    "mysents = tokenized[5:7]\n",
    "process_sentences_ne(mysents, ne_filter=None, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NE Type and Examples (returned when binary is set to False):**\n",
    "\n",
    "```\n",
    "ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "PERSON - Eddy Bonte, President Obama\n",
    "LOCATION - Murray River, Mount Everest\n",
    "DATE - June, 2008-06-29\n",
    "TIME - two fifty a m, 1:30 p.m.\n",
    "MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "PERCENT - twenty pct, 18.75 %\n",
    "FACILITY - Washington Monument, Stonehenge\n",
    "GPE - South East Asia, Midlothian\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing\n",
    "\n",
    "A very similar operation to stemming is called lemmatizing. The major difference between these is, as you saw earlier, stemming can often create non-existent words, whereas lemmas are actual words.\n",
    "\n",
    "Using from `nltk.stem import WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabbies ->  tabby\n",
      "cacti ->  cactus\n",
      "better ->  good\n",
      "best ->  best\n",
      "ran ->  ran\n",
      "ran ->  run\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print('tabbies -> ', lemmatizer.lemmatize('tabbies'))\n",
    "print('cacti -> ', lemmatizer.lemmatize('cacti'))\n",
    "print('better -> ', lemmatizer.lemmatize('better', pos='a'))\n",
    "print('best -> ', lemmatizer.lemmatize('best', pos='a'))\n",
    "print('ran -> ', lemmatizer.lemmatize('ran'))\n",
    "print('ran -> ', lemmatizer.lemmatize('ran', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to the corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   '[The King James Bible]\\n'\n",
      "    '\\n'\n",
      "    'The Old Testament of the King James Bible\\n'\n",
      "    '\\n'\n",
      "    'The First Book of Moses:  Called Genesis\\n'\n",
      "    '\\n'\n",
      "    '\\n'\n",
      "    '1:1 In the beginning God created the heaven and the earth.',\n",
      "    '1:2 And the earth was without form, and void; and darkness was upon\\n'\n",
      "    'the face of the deep.',\n",
      "    'And the Spirit of God moved upon the face of the\\nwaters.']\n"
     ]
    }
   ],
   "source": [
    "sample_text = gutenberg.raw(\"bible-kjv.txt\")\n",
    "tokenized = sent_tokenize(sample_text)\n",
    "pp.pprint(tokenized[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a list of the corpora see http://www.nltk.org/book/ch02.html#tab-corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet\n",
    "\n",
    "WordNet is a lexical database for the English language, which was created by Princeton, and is part of the NLTK corpus.\n",
    "\n",
    "You can use WordNet alongside the NLTK module to find the meanings of words, synonyms, antonyms, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n",
      "plan\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "syns = wordnet.synsets(\"program\")\n",
    "# Synset\n",
    "synset = syns[0]\n",
    "print(synset.name())\n",
    "# Just the word\n",
    "print(synset.lemmas()[0].name())\n",
    "# Definition\n",
    "print(synset.definition())\n",
    "# Examples\n",
    "print(synset.examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNONYMS:  {'salutary', 'honest', 'unspoilt', 'unspoiled', 'soundly', 'dependable', 'skilful', 'serious', 'well', 'adept', 'upright', 'trade_good', 'secure', 'effective', 'honorable', 'commodity', 'ripe', 'right', 'sound', 'skillful', 'estimable', 'in_effect', 'full', 'beneficial', 'safe', 'good', 'proficient', 'just', 'respectable', 'goodness', 'expert', 'in_force', 'practiced', 'thoroughly', 'dear', 'near', 'undecomposed'}\n",
      "ANTONYMS:  {'bad', 'badness', 'evilness', 'evil', 'ill'}\n"
     ]
    }
   ],
   "source": [
    "_synsets = wordnet.synsets('good')\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in _synsets:\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        ants = l.antonyms()\n",
    "        if ants:\n",
    "            antonyms.append(ants[0].name())\n",
    "            \n",
    "print('SYNONYMS: ', set(synonyms))\n",
    "print('ANTONYMS: ', set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can also easily use WordNet to compare the similarity of two words and their tenses, by incorporating the Wu and Palmer method for semantic related-ness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5263157894736842\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('sail.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('truck.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('cat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification and converting words to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In each category (pos/neg) take all file ids (reviews), and\n",
    "# store the tokenized words for that file id along with\n",
    "# pos/neg label:\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the documents for training and testing\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First document in the list\n",
    "print(documents[0])\n",
    "\n",
    "# Make a list of all words, lowercase\n",
    "all_words = [w.lower() for w in movie_reviews.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Frequency distribution\n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "# Top 15 words\n",
    "print(all_words.most_common(15))\n",
    "\n",
    "# Count of a word\n",
    "print(all_words['cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top someodd words\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "# Returns a dict with the word and its presence (bool) in document\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    features = {w: w in words for w in word_features}\n",
    "    return features\n",
    "\n",
    "print(find_features(movie_reviews.words('neg/cv000_29416.txt')))\n",
    "# List of words and their presence in document along with label for that document\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "print(featuresets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and saving naive Bayes classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy  =  0.66\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test set\n",
    "\n",
    "training_set, test_set = featuresets[:1900], featuresets[1900:]\n",
    "\n",
    "# Train\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# Test\n",
    "print(\"Classifier accuracy  = \", (nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               atrocious = True              neg : pos    =     11.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     10.4 : 1.0\n",
      "                    gump = True              pos : neg    =      9.3 : 1.0\n",
      "              incoherent = True              neg : pos    =      8.1 : 1.0\n",
      "                 layered = True              pos : neg    =      7.9 : 1.0\n",
      "               furniture = True              neg : pos    =      6.8 : 1.0\n",
      "                musicals = True              pos : neg    =      6.5 : 1.0\n",
      "                  skimpy = True              neg : pos    =      6.1 : 1.0\n",
      "                  osment = True              pos : neg    =      5.8 : 1.0\n",
      "                   flynt = True              pos : neg    =      5.8 : 1.0\n",
      "                  rebels = True              pos : neg    =      5.8 : 1.0\n",
      "            surveillance = True              neg : pos    =      5.5 : 1.0\n",
      "                   dreck = True              neg : pos    =      5.5 : 1.0\n",
      "                promptly = True              neg : pos    =      5.5 : 1.0\n",
      "                   vapid = True              neg : pos    =      5.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# What are the most valuable words when it comes to the neg/pos labels\n",
    "# You'll see the ratio of occurrences neg:pos or pos:neg, depending, to \n",
    "# see how often a word appears in one label as compared to the other.\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('naivebayes.pickle', 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('naivebayes.pickle', 'rb') as f:\n",
    "    classifier = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on one document (#5) without the label (just the first element of our tuple)\n",
    "classifier.classify(test_set[4][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK and Scikit-Learn\n",
    "There's an API made by the nltk folks for leveraging sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB accuracy =  0.69\n"
     ]
    }
   ],
   "source": [
    "mnb_classifer = SklearnClassifier(MultinomialNB())\n",
    "mnb_classifer.train(training_set)\n",
    "print('MultinomialNB accuracy = ',\n",
    "      nltk.classify.accuracy(mnb_classifer, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB accuracy =  0.65\n"
     ]
    }
   ],
   "source": [
    "bnb_classifer = SklearnClassifier(BernoulliNB())\n",
    "bnb_classifer.train(training_set)\n",
    "print('BernoulliNB accuracy = ',\n",
    "      nltk.classify.accuracy(bnb_classifer, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression accuracy =  0.71\n"
     ]
    }
   ],
   "source": [
    "lr_classifier = SklearnClassifier(LogisticRegression())\n",
    "lr_classifier.train(training_set)\n",
    "print('LogisticRegression accuracy = ',\n",
    "      nltk.classify.accuracy(lr_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier accuracy =  0.59\n"
     ]
    }
   ],
   "source": [
    "sgd_classifier = SklearnClassifier(SGDClassifier())\n",
    "sgd_classifier.train(training_set)\n",
    "print('SGDClassifier accuracy = ',\n",
    "      nltk.classify.accuracy(sgd_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC accuracy =  0.36\n"
     ]
    }
   ],
   "source": [
    "svc_classifier = SklearnClassifier(SVC())\n",
    "svc_classifier.train(training_set)\n",
    "print('SVC accuracy = ',\n",
    "      nltk.classify.accuracy(svc_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC accuracy =  0.65\n"
     ]
    }
   ],
   "source": [
    "linearsvc_classifier = SklearnClassifier(LinearSVC())\n",
    "linearsvc_classifier.train(training_set)\n",
    "print('LinearSVC accuracy = ',\n",
    "      nltk.classify.accuracy(linearsvc_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC accuracy =  0.64\n"
     ]
    }
   ],
   "source": [
    "nusvc_classifier = SklearnClassifier(NuSVC())\n",
    "nusvc_classifier.train(training_set)\n",
    "print('NuSVC accuracy = ',\n",
    "      nltk.classify.accuracy(nusvc_classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "        \n",
    "    def classify(self, features):\n",
    "        \"\"\"Iterates through classifiers and classifies\n",
    "           based on features, returning the most popular \"vote\"\n",
    "           or classification.\"\"\"\n",
    "        votes = [c.classify(features) for c in self._classifiers]\n",
    "        try:\n",
    "            m = mode(votes)\n",
    "        except Exception as e:\n",
    "            m = None\n",
    "        return m\n",
    "    \n",
    "    def vote_ratio(self, features):\n",
    "        \"\"\"For or against.\"\"\"\n",
    "        votes = [c.classify(features) for c in self._classifiers]\n",
    "        try:\n",
    "            vote_ratio = votes.count(mode(votes)) / len(votes)\n",
    "        except Exception as e:\n",
    "            vote_ratio = None\n",
    "        return vote_ratio\n",
    "\n",
    "def find_features(document):\n",
    "    \"\"\"Returns a dict with the word and its presence (bool) in document\"\"\"\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    features = {w: w in words for w in word_features}\n",
    "    return features\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy  =  0.64\n",
      "MultinomialNB accuracy =  0.7\n",
      "BernoulliNB accuracy =  0.64\n",
      "LogisticRegression accuracy =  0.67\n",
      "SGDClassifier accuracy =  0.65\n",
      "SVC accuracy =  0.48\n",
      "LinearSVC accuracy =  0.63\n",
      "NuSVC accuracy =  0.67\n",
      "voted_classifier accuracy =  0.58\n",
      "Classification: neg Vote ratio: 0.75\n",
      "Classification: None Vote ratio: None\n",
      "Classification: pos Vote ratio: 0.875\n",
      "Classification: pos Vote ratio: 1.0\n",
      "Classification: neg Vote ratio: 0.875\n",
      "Classification: neg Vote ratio: 0.875\n"
     ]
    }
   ],
   "source": [
    "     \n",
    "# In each category (pos/neg) take all file ids (reviews), and\n",
    "# store the tokenized words for that file id along with\n",
    "# pos/neg label:\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the documents for training and testing\n",
    "random.shuffle(documents)\n",
    "\n",
    "# List of words in movie reviews\n",
    "all_words = [w.lower() for w in movie_reviews.words()]\n",
    "\n",
    "# Frequency distribution\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "# Top someodd words\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "# List of words and their presence in document along with label for that document\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "# Split into test and training data sets\n",
    "training_set, test_set = featuresets[:1900], featuresets[1900:]\n",
    "\n",
    "# Reload model\n",
    "with open('naivebayes.pickle', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "    \n",
    "\n",
    "# Train and test\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier accuracy  = \", (nltk.classify.accuracy(classifier, test_set)))\n",
    "\n",
    "\n",
    "mnb_classifer = SklearnClassifier(MultinomialNB())\n",
    "mnb_classifer.train(training_set)\n",
    "print('MultinomialNB accuracy = ',\n",
    "      nltk.classify.accuracy(mnb_classifer, test_set))\n",
    "                                  \n",
    "bnb_classifer = SklearnClassifier(BernoulliNB())\n",
    "bnb_classifer.train(training_set)\n",
    "print('BernoulliNB accuracy = ',\n",
    "      nltk.classify.accuracy(bnb_classifer, test_set))\n",
    "\n",
    "lr_classifier = SklearnClassifier(LogisticRegression())\n",
    "lr_classifier.train(training_set)\n",
    "print('LogisticRegression accuracy = ',\n",
    "      nltk.classify.accuracy(lr_classifier, test_set))\n",
    "\n",
    "sgd_classifier = SklearnClassifier(SGDClassifier())\n",
    "sgd_classifier.train(training_set)\n",
    "print('SGDClassifier accuracy = ',\n",
    "      nltk.classify.accuracy(sgd_classifier, test_set))\n",
    "                                  \n",
    "svc_classifier = SklearnClassifier(SVC())\n",
    "svc_classifier.train(training_set)\n",
    "print('SVC accuracy = ',\n",
    "      nltk.classify.accuracy(svc_classifier, test_set))\n",
    "                                  \n",
    "linearsvc_classifier = SklearnClassifier(LinearSVC())\n",
    "linearsvc_classifier.train(training_set)\n",
    "print('LinearSVC accuracy = ',\n",
    "      nltk.classify.accuracy(linearsvc_classifier, test_set))\n",
    "                                  \n",
    "nusvc_classifier = SklearnClassifier(NuSVC())\n",
    "nusvc_classifier.train(training_set)\n",
    "print('NuSVC accuracy = ',\n",
    "      nltk.classify.accuracy(nusvc_classifier, test_set))\n",
    "\n",
    "# Voting\n",
    "\n",
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  mnb_classifer,\n",
    "                                  bnb_classifer,\n",
    "                                  lr_classifier,\n",
    "                                  sgd_classifier,\n",
    "                                  svc_classifier,\n",
    "                                  linearsvc_classifier,\n",
    "                                  nusvc_classifier)\n",
    "\n",
    "print('voted_classifier accuracy = ', (nltk.classify.accuracy(voted_classifier, test_set)))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[0][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[0][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[1][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[1][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[2][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[2][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[3][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[3][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[4][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[4][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[5][0]), \"Vote ratio:\",voted_classifier.vote_ratio(test_set[5][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
